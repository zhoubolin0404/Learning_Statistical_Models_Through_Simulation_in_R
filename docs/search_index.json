[["index.html", "学习统计模型——通过R模拟 概述 如何引用本书 发现问题？ 教育者需知", " 学习统计模型——通过R模拟 Zhou Bolin 2024-06-03 概述 本教材采用在R语言环境下模拟广义线性模型(General Linear Model, GLM)的方法来介绍统计分析。总体目标是教会学生如何将研究设计的描述转化为线性模型，来分析该研究的数据。重点是分析心理学实验数据所需的技能。 本教材包括以下内容： 线性模型工作流程; 方差-协方差矩阵; 多元回归; 交互作用(连续与分类; 分类与分类); 线性混合模型; 广义线性混合模型。 本课程的内容构成了格拉斯哥大学心理学院(University of Glasgow School of Psychology)由Dale Barr讲授的大学三年级一学期课程的基础。这也是由格拉斯哥大学心理学院工作人员开发的PsyTeachR系列课程材料的一部分。 与你可能遇到的其他教材不同，这是一本互动教材。每一章都包含嵌入式练习和网页应用来帮助学生更好地理解内容。你只有通过浏览器访问这些材料，这些互动内容才能正常工作。因此，不建议打印这些材料。如果你希望在没网的情况下访问教材或保存本地版本以防止网站变化或迁移，可以下载离线使用版本。只需要从ZIP压缩包中提取文件，在docs目录中找到index.html文件，然后使用浏览器打开这个文件即可。 如何引用本书 Barr, Dale J. (2021). Learning statistical models through simulation in R: An interactive textbook. Version 1.0.0. Retrieved from https://psyteachr.github.io/stat-models-v1. 发现问题？ 如果你发现错误或书写错误，有问题或建议，请在https://github.com/psyteachr/stat-models-v1/issues提交问题。谢谢！ 教育者需知 您可以根据自己的需求免费重复使用和修改本教材中的材料，但需要注明原作出处。请注意关于重复使用本材料的 Creative Commons CC-BY-SA 4.0 许可证的其他条款。 本书是使用R bookdown包构建的。源文件可在github上获得。 "],["前言.html", "1 前言 1.1 课程目的 1.2 广义线性混合模型(Generalized Linear Mixed-Effects Models, GLMMs) 1.3 关于可重复性的说明 1.4 基于模拟的方法", " 1 前言 1.1 课程目的 本课程的目的是教你如何分析(还有模拟!)作为心理学家你可能遇到的各种数据集。重点是行为数据——反应时、知觉判断、选择、决策、李克特量表评分、眼动、睡眠时间等。这些数据通常是在有计划的研究或实验中收集到的。 本课程旨在教授灵活、可推广、可重复的分析技术。你将学习到的技术是灵活的，这意味着它们可以应用到各式各样的研究设计和不同类型的数据上。通过充分考虑抽样对统计推断的潜在偏倚影响，它们在最大程度上具有可推广性——这有助于支持超越特定被试和实验中涉及的刺激得出结论。最后，你将学习到的技术会尽可能地做到可完全重复，因为你的分析将以R代码的纯文本脚本形式明确记录从原始数据到研究结果的每一个步骤。 1.2 广义线性混合模型(Generalized Linear Mixed-Effects Models, GLMMs) 本课程强调一个灵活的回归模型框架而不是教授处理不同类型数据的”公式”。课程假设你需要分析的基本数据类型将是多层的(multilevel)，而且你不仅仅需要处理连续测量数据，还要处理有序测量数据（李克特量表评分）、计数数据（特定事件发生的次数）以及名义数据（某物所属的类别）。 本课程结束时，你将学会如何使用广义线性混合模型(GLMMs)来量化因变量和一组预测变量之间的关系。要理解GLMMs，你需要学习以下三部分： “线性模型”部分，包括如何捕捉不同类型的预测变量及其交互作用； “混合”部分，包括如何使用随机效应来表示通过对同一批被试或刺激进行重复测量而产生的多层次依赖关系； “广义”部分，包括拓展线性模型来表示非完全正态自变量，包括计数、有序和二元变量。 1.2.1 线性模型 GLMMs是一般线性模型的扩展。一般线性模型是方差分析(ANOVA)、t检验和古典回归等更简单方法的基础。本课程的主要观点是：你可能遇到的几乎所有实验得到的数据都可以用GLMMs进行分析。 一个线性模型的简单例子： \\[Y_i = \\beta_0 + \\beta_1 X_i + e_i\\] 其中\\(Y_i\\)是样本\\(i\\)的因变量的观测值，由截距加上被系数\\(\\beta_1\\)加权的预测值\\(X_i\\)以及误差项构成。表示线性关系\\(Y_i = 3 + 2X_i + e_i\\)的模拟数据如图1.1所示。 图1.1: 线性模型Y = 3 + 2X的模拟数据 library(&quot;tidyverse&quot;) # if needed set.seed(62) dat &lt;- tibble(X = runif(100, -3, 3), Y = 3 + 2 * X + rnorm(100)) ggplot(dat, aes(X, Y)) + geom_point() + geom_abline(intercept = 3, slope = 2, color = &quot;blue&quot;) 你可能会发现上述方程表示的是一条直线(\\(y = mx + b\\))，其中\\(\\beta_0\\)是截距，\\(\\beta_1\\)是斜率。\\(e_i\\)是样本\\(i\\)的模型误差，表示样本观测值\\(Y_i\\)与给定\\(X_i\\)的模型预测值之间的差距。 表示法惯例 希腊字母(\\(\\beta\\), \\(\\rho\\), \\(\\tau\\))表示总体参数，通常是未观测到的，需要从数据中估计得到的。当我们想要区分估计参数和真实值时，我们会使用”hat”：如\\(\\hat{\\beta}_0\\)表示从数据中估计得到的\\(\\beta_0\\)的值。 大写拉丁字母(\\(X\\)、\\(Y\\))表示观测值——即你已经测量过的值，因此是已知的。你也会看到小写拉丁字母(如\\(e_i\\))，表示统计误差或其他我将称之为派生量或虚拟量的东西(这将在课程后面进行解释)。 线性模型中的”线性”并不像你想象的那样! 许多人认为”线性模型”只能捕捉线性关系，即可以用直线(或平面)来描述的关系。这是错误的。 线性模型是各种项的加权和，每个项都有一个预测变量(或常数)乘上一个系数。在上述模型中，系数是\\(\\beta_0\\)和\\(\\beta_1\\)。你可以用线性模型拟合各种复杂的关系，包括非线性关系，如下所示。 图1.2: 用线性模型建模的非线性关系 在左边面板中，我们使用线性模型\\(Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2\\)捕捉了一个二次(抛物线)函数。X和Y之间的关系是非线性的，但模型本身是线性的。我们对预测变量\\(X\\)进行了平方处理，但系数\\(\\beta_0\\)、\\(\\beta_1\\)和\\(\\beta_2\\)并没有被平方、立方或类似的处理(它们都是”一次幂”)。 在中间面板中，我们使用线性模型捕捉了一个S形函数。在这里，Y变量表示某个事件的概率，例如基于学习时间来预测通过考试的概率。在这种情况下，我们通过在一个特殊的转换空间中估计线性模型来建模X和Y之间的关系，使得X-Y关系是线性的，然后将模型投影回非线性的概率空间(使用”链接函数”)。非线性来自于”链接函数”，但模型本身是线性的。 最后，右边面板展示了一种略显任意的波动模式，由广义可加混合模型捕获——这是一种我们在本课程中不会学到的高级技术。但从根本上说，它仍然是一个线性模型，因为它还是一系列复杂事物(在这种情况下是”基础函数”)的加权和，而系数提供了权重。 线性模型是一个系数是线性的模型；模型中的每个系数(\\(\\beta_0\\)、\\(\\beta_1\\))只允许被设置为一次幂，并且每个项\\(\\beta_i X_j\\)都只涉及单个系数。这些项只能与涉及其他系数的项相加，但不能相乘或相除(如\\(Y = \\frac{\\beta_1}{\\beta_2} X\\)是不允许的)。 当前课程的一个局限性是主要关注单变量数据，即将单一响应变量作为分析的焦点。通常情况下，你会处理相同对象的多个响应变量，但是同时对它们进行建模在技术上是非常困难的，并且超出了本课程的范围。一种更简单的方法(也是这里采用的方法)是对每个响应变量进行单独的单变量分析。 1.2.2 混合模型 研究结果的推断或解释的可推广性指的是它能够被轻松应用于超出特定研究背景(对象、刺激、任务等)的情境的程度。最理想的情况是，我们的发现能够适用于人类物种的所有成员，涵盖各种各样的刺激和任务；最糟糕的情况是，它们只能适用于那些受到我们使用的特定刺激的特定的人，在我们研究的特定背景下才观察得到。 研究结果的可推广性取决于几个因素：研究的设计方式、所使用的材料、被试的招募方式、给予被试的任务的性质以及数据分析的方式。在这门课程中我们将重点关注最后一点。在分析一个数据集时，如果你想要提出具有推广性的论断，你必须决定哪些可以算作你研究的重复——关于哪些方面应该在复制中保持不变，以及哪些方面允许变化。 不幸的是，有时你会发现数据以一种不太支持广泛意义上的可推广性的方式进行分析，这往往是因为低估了刺激材料或实验任务的独特特征对观测结果的影响 (Yarkoni, 2019)。 1.3 关于可重复性的说明 本课程的数据分析是使用R编写脚本进行的。 可重复性指的是在不同情况下重现研究结果的可能性程度。 如果我们能在给定原始数据的情况下得到相同的结果，我们会说这个发现在分析(analytically)或计算(computationally)上是可重复的。需要注意的是，这与说一个发现可复制(replicable)是不同的。可复制指的是能够在新样本中复制这一发现。对于这些术语并没有广泛的共识，但方便起见，我们可以将分析上的可重复性(reproducibility)和可复制性(replicability)视为两种不同但相关的可重复性(reproducibility)类型，前者反映分析员之间(或同一分析员随时间变化)的可重复性，而后者反映了在被试样本或亚群体之间的可重复性。 确保分析可重复性是一个难题。如果你未能正确记录自己的分析过程，或者你使用的软件被修改或过时并且变得不可用，你可能会在重现自己的发现时遇到麻烦！ 分析的另一个重要属性是透明度——在某种研究中所有步骤都可以公开的程度。一项研究可能是透明的但不可重复，反之亦然。使用促进透明度的工作流程非常重要。这使得脚本编程的“编辑–执行”工作流程对于数据分析来说是理想的选择，远远优于大多数商业统计软件的“指向–点击”的工作流程。通过编写代码，你可以使逻辑和决策过程对他人明确，并易于重建。 1.4 基于模拟的方法 本课程最后一个重要特点是采用了基于模拟的方法来学习统计模型。通过数据模拟，我们定义一个特定模型来描述感兴趣的总体，然后利用计算机的随机数生成器来模拟从该总体中抽样的过程。我们将在下面看一个简单的例子。 在分析数据时，你会面临的经典问题是你不知道你正在研究的总体的“真实情况”。你从该总体中抽取一个样本，对观测到的数据获取方式做出一些假设，然后利用观察到的数据来估计未知的总体参数及这些参数的不确定性。 数据模拟颠倒了这个过程。你会定义一个模型的参数，代表关于(假设的)总体的真实情况，从中获取数据。然后，你可以像平常一样分析获得的数据，并考察参数估计与真实值之间的对应程度。 让我们看一个例子。假设你对以下问题感兴趣：作为学步幼儿的父母是否会“提高”你的反应能力。如果你曾经照顾过一个幼儿，你就会知道身体危险似乎总是在即——他们可能从刚刚爬上的椅子上摔下来，被门里夹到，头撞在桌子角上等等——所以你需要保持警惕并准备迅速行动。你假设这种警惕会转化为在幼儿不在场的其他情况下的更快反应时间，比如在心理实验室里。因此，你招募了一组有幼儿的父母来实验室。你让每个父母在闪烁的灯光出现时尽快按下按钮，并测量他们的反应时(以毫秒为单位)。对于每个父母，你计算了他们在所有试验中的平均反应时。我们可以使用R中的rnorm()函数模拟50个父母的平均反应时。但在我们开始之前，我们将加载我们需要的包（tidyverse）并设置随机种子(random seed)，以确保你(读者)得到和我(作者)相同的随机值。 library(&quot;tidyverse&quot;) set.seed(2021) # 可以是任意整数 parents &lt;- rnorm(n = 50, mean = 480, sd = 40) ## [1] 475.1016 502.0983 493.9460 494.3853 515.9221 403.0972 490.4698 516.6227 ## [9] 480.5509 549.1985 436.7118 469.0870 487.2798 540.3417 544.1788 406.3410 ## [17] 544.9324 485.2556 539.2449 540.5327 442.3023 472.5726 435.9550 528.3246 ## [25] 415.0025 484.2151 421.7823 465.8394 476.2520 524.0267 401.4470 422.0822 ## [33] 520.7777 423.1433 455.8187 416.6610 428.5627 421.8126 476.5172 500.1895 ## [41] 484.6555 550.4085 466.1953 564.8000 478.6249 448.3138 539.0206 450.9777 ## [49] 492.4952 507.6786 我们选择使用rnorm()来生成数据，这是一个从正态分布中生成随机数的函数，这反映了我们的假设——平均反应时在总体中呈正态分布。正态分布由两个参数定义，一个是均值(通常用希腊字母\\(\\mu\\)表示，发音为”myoo”)，另一个是标准差(通常用希腊字母\\(\\sigma\\)表示，发音为”sigma”)。由于我们自己生成了数据，所以\\(\\mu\\)和\\(\\sigma\\)都是已知的，在调用rnorm()时，我们将它们分别设置为480和40。 当然，为了验证我们的假设，我们需要一个对照组，所以我们定义了一个非父母的对照组。我们用相同的方式从这个对照组生成数据，但改变了平均值。 control &lt;- rnorm(n = 50, mean = 500, sd = 40) 让我们将它们放入数据框(tibble)中，以便更容易绘制和分析数据。该数据框中的每一行表示来自特定被试的平均反应时。 dat &lt;- tibble(group = rep(c(&quot;parent&quot;, &quot;control&quot;), each = 50), rt = c(parents, control)) dat ## # A tibble: 100 × 2 ## group rt ## &lt;chr&gt; &lt;dbl&gt; ## 1 parent 475. ## 2 parent 502. ## 3 parent 494. ## 4 parent 494. ## 5 parent 516. ## 6 parent 403. ## 7 parent 490. ## 8 parent 517. ## 9 parent 481. ## 10 parent 549. ## # ℹ 90 more rows 下面是对模拟数据的一些尝试。 以某种合理的方式绘制数据。 计算平均值和标准差。它们与总体参数相比如何? 对这些数据进行t检验。群组效应显著吗? 做完这些后，再做一次，但改变样本量、总体参数或两者都改变。 参考文献 Yarkoni, T. (2019). The generalizability crisis. https://doi.org/10.31234/osf.io/jqw35 "],["相关和回归.html", "2 相关和回归 2.1 相关矩阵(Correlation matrices) 2.2 模拟二元数据 2.3 相关和回归的关系 2.4 练习", " 2 相关和回归 2.1 相关矩阵(Correlation matrices) 你可能已经通过阅读心理学论文对相关矩阵这个概念有所了解。相关矩阵是总结同一个体的多个测量值之间关系的常用方法。 假设你用多个量表测量了心理幸福感。一个问题是这些量表在多大程度上测量了相同的东西。通常，你会查看相关矩阵来探索测量之间所有成对关系。 回忆一下，相关系数量化了两个变量之间关系的强度和方向。它通常用符号\\(r\\)或\\(\\rho\\)(希腊字母”rho”)表示。相关系数的范围在-1到1之间，其中0表示没有关系，正值反映正相关(一个变量增加，另一个也增加)，负值反映负相关(一个变量增加，另一个减少)。 图2.1: 不同类型的二元关系 如果你有\\(n\\)个测量值，你可以计算多少个成对相关？你可以用下面蓝框中的公式来计算，也可以更简单地通过R中的choose(n, 2)函数直接计算。例如，要获得6个测量值之间可能的成对相关数量，你可以输入choose(6, 2)，这会告诉你有15对组合。 对于任意\\(n\\)个测量值，你可以计算\\(\\frac{n!}{2(n - 2)!}\\)个各值之间的成对相关。符号\\(!\\)称为阶乘(factorial)运算符，定义为从1到\\(n\\)所有数字的乘积。因此，如果你有6个测量值，你可以得到 \\[ \\frac{6!}{2(6-2)!} = \\frac{1 \\times 2 \\times 3 \\times 4 \\times 5 \\times 6}{2\\left(1 \\times 2 \\times 3 \\times 4\\right)} = \\frac{720}{2(24)} = 15 \\] 你可以使用R中的base::cor()或corrr::correlate()来创建相关矩阵。我们更喜欢后者函数，因为cor()要求你的数据存储在矩阵中，而我们将处理的大多数数据是存储在数据框中的表格数据。corrr::correlate()函数将数据框作为第一个参数，并提供”整洁”的输出，因此它可以更好地与tidyverse系列函数和管道操作符(%&gt;%)联动。 让我们创建一个相关矩阵来看看它是如何工作的。首先加载我们需要的包。 library(&quot;tidyverse&quot;) library(&quot;corrr&quot;) # 如缺失(missing)，在控制台(console)中输入install.packages(&quot;corrr&quot;) 我们将使用starwars数据集，这是在加载tidyverse包后可用的内置数据集。该数据集包含了出现在星球大战电影系列中的各种角色的信息。让我们来看看之间的相关性 starwars %&gt;% select(height, mass, birth_year) %&gt;% correlate() ## Correlation computed with ## • Method: &#39;pearson&#39; ## • Missing treated using: &#39;pairwise.complete.obs&#39; ## # A tibble: 3 × 4 ## term height mass birth_year ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 height NA 0.131 -0.404 ## 2 mass 0.131 NA 0.478 ## 3 birth_year -0.404 0.478 NA 你可以在任意给定的行或列的交叉处查找任何双变量相关系数。因此，height和mass之间的相关系数是.131，你可以在第1行，第2列或第2行，第1列找到它——它们是相同的。请注意，这里只有choose(3, 2) = 3个唯一的双变量关系，但每个关系在表中出现了两次。我们可能只想显示唯一的组合，这可以通过在管道中附加corrr::shave()来实现。 starwars %&gt;% select(height, mass, birth_year) %&gt;% correlate() %&gt;% shave() ## Correlation computed with ## • Method: &#39;pearson&#39; ## • Missing treated using: &#39;pairwise.complete.obs&#39; ## # A tibble: 3 × 4 ## term height mass birth_year ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 height NA NA NA ## 2 mass 0.131 NA NA ## 3 birth_year -0.404 0.478 NA 现在我们只有相关矩阵的下三角部分，但NA看起来很难看，前导0也不美观。corrr包还提供了fashion()函数，可以对其进行清理(更多选项请查阅?corrr::fashion)。 starwars %&gt;% select(height, mass, birth_year) %&gt;% correlate() %&gt;% shave() %&gt;% fashion() ## Correlation computed with ## • Method: &#39;pearson&#39; ## • Missing treated using: &#39;pairwise.complete.obs&#39; ## term height mass birth_year ## 1 height ## 2 mass .13 ## 3 birth_year -.40 .48 相关性只有在关系(大致)线性且没有严重的异常值对结果产生过大影响时才能很好地描述关系。因此，可视化相关性通常和量化它们一样是个好主意。base::pairs()函数可以实现这一点。pairs()的第一个参数形式为~ v1 + v2 + v3 + ... + vn，其中v1、v2等是你想要进行相关分析的变量名。 pairs(~ height + mass + birth_year, starwars) 图2.2: 星球大战数据集相关关系 我们会发现一个巨大的离群值影响了我们的数据。具体来说是有个体重超过1200kg的生物。让我们找出它并从数据集里面删掉它。 starwars %&gt;% filter(mass &gt; 1200) %&gt;% select(name, mass, height, birth_year) ## # A tibble: 1 × 4 ## name mass height birth_year ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Jabba Desilijic Tiure 1358 175 600 好了，让我们看看没有了这个庞然大物的数据会是什么样子。 starwars2 &lt;- starwars %&gt;% filter(name != &quot;Jabba Desilijic Tiure&quot;) pairs(~height + mass + birth_year, starwars2) 图2.3: 去除体重离群值后星球大战数据集相关关系 好多了，但还有个生物的离群出生年份可能是我们不想要的。 starwars2 %&gt;% filter(birth_year &gt; 800) %&gt;% select(name, height, mass, birth_year) ## # A tibble: 1 × 4 ## name height mass birth_year ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Yoda 66 17 896 是尤达大师！他和宇宙一样古老。让我们抛开他看看图会怎么样。 starwars3 &lt;- starwars2 %&gt;% filter(name != &quot;Yoda&quot;) pairs(~height + mass + birth_year, starwars3) 图2.4: 去除体重和出生年份离群值后星球大战数据集相关关系 看起来更好了。让我们看看它是怎样改变我们的相关矩阵的。 starwars3 %&gt;% select(height, mass, birth_year) %&gt;% correlate() %&gt;% shave() %&gt;% fashion() ## Correlation computed with ## • Method: &#39;pearson&#39; ## • Missing treated using: &#39;pairwise.complete.obs&#39; ## term height mass birth_year ## 1 height ## 2 mass .73 ## 3 birth_year .44 .24 请注意，这些值与我们开始时的值有很大不同。 有时移除离群值不是一个好办法。处理离群值的另一个办法是使用一种更稳健(robust)的方法。使用corrr::correlate()默认计算的相关系数是Pearson积差相关(Pearson product-moment correlation)系数。我们也能通过改变correlate()的method()参数来计算Spearman相关系数。这将在计算相关性之前用排名替换原始值，因此仍会包括离群值，但影响将大大减小。 starwars %&gt;% select(height, mass, birth_year) %&gt;% correlate(method = &quot;spearman&quot;) %&gt;% shave() %&gt;% fashion() ## Correlation computed with ## • Method: &#39;spearman&#39; ## • Missing treated using: &#39;pairwise.complete.obs&#39; ## term height mass birth_year ## 1 height ## 2 mass .72 ## 3 birth_year .15 .15 顺便一提，如果你用R Markdown生成报告，并希望你的表格有个好看的格式，可以使用knitr:: able()。 starwars %&gt;% select(height, mass, birth_year) %&gt;% correlate(method = &quot;spearman&quot;) %&gt;% shave() %&gt;% fashion() %&gt;% knitr::kable() term height mass birth_year height mass .72 birth_year .15 .15 2.2 模拟二元数据 你已经学会了使用rnorm()函数从正态分布中模拟数据。回忆一下，rnorm()允许你指定单个变量的均值和标准差。那我们怎么模拟相关变量呢？ 应该很明确，你不能仅仅运行两次rnorm()后组合变量就完事。因为这会得到两个不相关的变量，即相关性为零。 MASS包提供mvrnorm()函数，这是rnorm的”多元”(multivariate)版(因此函数的名字是’mv’ + ‘rnorm’，这样更容易记住)。 R预装了MASS包。但MASS包中你唯一可能会用到的函数只是mvrnorm()，因此相比于使用library(\"MASS\")加载包，使用MASS::mvrnorm()是更好的办法，尤其是在MASS和tidyverse里的dplyr包不太合得来的情况下(因为两个包都有select()函数)。因此，如果在加载tidyverse之后加载MASS，那么最终得到的select()是MASS版本，而不是dplyr版本。这会让你绞尽脑汁来找出代码的问题所在，所以总在不加载的情况下使用MASS::mvrnorm()吧。 这里作者贴了一则他在Twitter(现X)上吐槽MASS的打油诗，不过现在已经查不到了QwQ，考虑到翻译水平不佳，附上原文！ MASS before dplyr, clashes not dire; dplyr before MASS, pain in the ass. —— Dale Barr(September 30, 2014) 请查看mvrnorm()函数的文档(在控制台输入?MASS::mvrnorm)。 有3个参数需要注意： 参数 描述 n 所需样本数 mu 一个给出变量均值的向量 Sigma 一个正定对称矩阵，用于指定变量的协方差矩阵 对n和mu的描述可以理解，但“一个正定对称矩阵(positive-definite symmetric matrix)，用于指定变量的协方差矩阵”是什么意思呢？ 当你有个多元数据时，协方差矩阵(也叫方差-协方差矩阵)反映了各个变量的方差及其相互关系。它类似于标准差的多维版本。要充分描述单变量正态分布，你只需要知道均值和标准差；要描述双变量正态分布，你需要分别知道两个变量的均值、标准差和他们的相关性；对于包含两个以上变量的多元分布，你需要知道所有变量的均值、它们的标准差以及所有可能的成对相关性。这些概念在我们开始讨论混合效应模型时会变得非常重要。 你可以将协方差矩阵看作类似于之前见过的相关矩阵；实际上，通过一些计算，你可以将协方差矩阵转换为相关矩阵。 你在说什么《黑客帝国(Matrix)》？那不是从上世纪90年代开始的科幻电影系列吗？ 在数学中，矩阵只是向量概念的推广:向量被认为只有一个维度，而矩阵可以是任意数量的维度。 那么矩阵 \\[ \\begin{pmatrix} 1 &amp; 4 &amp; 7 \\\\ 2 &amp; 5 &amp; 8 \\\\ 3 &amp; 6 &amp; 9 \\\\ \\end{pmatrix} \\] 是一个3(行) x 3(列)矩阵，包括了列向量\\(\\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\\\ \\end{pmatrix}\\)，\\(\\begin{pmatrix} 4 \\\\ 5 \\\\ 6 \\\\ \\end{pmatrix}\\)和\\(\\begin{pmatrix} 7 \\\\ 8 \\\\ 9 \\\\ \\end{pmatrix}\\)。通常我们用\\(i\\) x \\(j\\)的形式表示矩阵，\\(i\\)是行数，\\(j\\)是列数。因此，一个3x2矩阵有3行2列，像这样： \\[ \\begin{pmatrix} a &amp; d \\\\ b &amp; e \\\\ c &amp; f \\\\ \\end{pmatrix} \\] 方阵是行数等于列数的矩阵。 你可以用matrix()函数在R中创建一个方阵，或者使用base R的 cbind()和rbind()将向量连接在一起，它们分别将向量按列和按行连接在一起。在控制台试试cbina(1:3, 4:6, 7:9)吧。 那么“正定”和“对称”是什么呢？这是对可以表示多元正态分布的这类矩阵的数学要求。换句话说，你提供的协方差矩阵必须表示一个合法的多元正态分布。在这点上，你真的不需要知道再多了。 让我们从模拟假设的人类身高和体重的数据开始。我们知道这些是相关的。为了能够模拟数据，我们需要这两个变量的均值和标准差以及它们的相关性。 我找到了一些数据并把它转换为CSV文件。如果你想跟上，可以下载这个文件heights_and_weights.csv。这是散点图： handw &lt;- read_csv(&quot;data/heights_and_weights.csv&quot;, col_types = &quot;dd&quot;) ggplot(handw, aes(height_in, weight_lbs)) + geom_point(alpha = .2) + labs(x = &quot;height (inches)&quot;, y = &quot;weight (pounds)&quot;) 图2.5: 475人的身高和体重(包括婴儿) 这不是一个线性关系。我们可以先对每个变量进行对数(log)变换。 handw_log &lt;- handw %&gt;% mutate(hlog = log(height_in), wlog = log(weight_lbs)) 图2.6: 对数变换后的身高和体重 散点图右上侧尾部有一个大的点簇，这可能表示在这个样本中成年人比儿童更多，因为成年人更高和更重。 对数身高的均值是4.11 (SD = 0.26)，而对数体重的均值是4.74 (SD = 0.65)。对数身高和对数体重之间的相关性我们可以用cor()函数获得，高达0.96。 我们现在有了模拟500人身高和体重所需的所有信息。但我们如何将这些信息输入到MASS::mvrnorm()呢？我们知道函数调用的第一部分是MASS::mvrnorm(500, c(4.11,4.74), ...)，但Sigma——那个协方差矩阵呢？我们从上面知道\\(\\hat{\\sigma}_x = 0.26\\)和\\(\\hat{\\sigma}_y = 0.65\\)，以及\\(\\hat{\\sigma}_y = 0.65\\), and \\(\\hat{\\rho}_{xy} = 0.96\\)。 表示二元数据Sigma (\\(\\mathbf{\\Sigma}\\))的协方差矩阵如下： \\[ \\mathbf{\\Sigma} = \\begin{pmatrix} {\\sigma_x}^2 &amp; \\rho_{xy} \\sigma_x \\sigma_y \\\\ \\rho_{yx} \\sigma_y \\sigma_x &amp; {\\sigma_y}^2 \\\\ \\end{pmatrix} \\] 方差（标准差的平方，\\({\\sigma_x}^2\\)和\\({\\sigma_y}^2\\)）位于对角线上，协方差(相关系数乘以两个标准差，\\(\\rho_{xy} \\sigma_x \\sigma_y\\))位于非对角线上。记住协方差就是相关系数与两个标准差的积，这是很有用的。正如我们在上面的相关矩阵中看到的，表格中存在额外信息；也就是说，协方差同时出现在矩阵的右上角单元格和左下角单元格。 代入之前的值，协方差矩阵应该是： \\[ \\begin{pmatrix} .26^2 &amp; (.96)(.26)(.65) \\\\ (.96)(.65)(.26) &amp; .65^2 \\\\ \\end{pmatrix} = \\begin{pmatrix} .067 &amp; .162 \\\\ .162 &amp; .423 \\\\ \\end{pmatrix} \\] 很好，那么我们如何在R中形成Sigma以便我们可以将它传递给mvrnorm()函数呢？我们将使用matrix()函数，如下所示。 首先，让我们定义协方差并将其存储在变量my_cov中。 my_cov &lt;- .96 * .26 * .65 现在我们将使用matrix()来定义我们的Sigma为my_Sigma。 my_Sigma &lt;- matrix(c(.26^2, my_cov, my_cov, .65^2), ncol = 2) my_Sigma ## [,1] [,2] ## [1,] 0.06760 0.16224 ## [2,] 0.16224 0.42250 对matrix()函数感到困惑吗? 通过运行下面的代码，你可以发现matrix()是逐列填充矩阵元素，而不是逐行填充: matrix(c(\"a\", \"b\", \"c\", \"d\"), ncol = 2) 如果你想改变这种行为，将byrow参数设置为TRUE。 matrix(c(\"a\", \"b\", \"c\", \"d\"), ncol = 2, byrow = TRUE) 太好了！现在我们得到了my_Sigma，我们已经准备好使用MASS::mvrnorm()了。让我们通过创建6个模拟的人的数据来测试一下。 set.seed(62) # 为了可重复性 # 传递*命名的*向量c(height = 4.11, weight = 4.74)给mu可以在输出中得到列名 log_ht_wt &lt;- MASS::mvrnorm(6, c(height = 4.11, weight = 4.74), my_Sigma) log_ht_wt ## height weight ## [1,] 4.254209 5.282913 ## [2,] 4.257828 4.895222 ## [3,] 3.722376 3.759767 ## [4,] 4.191287 4.764229 ## [5,] 4.739967 6.185191 ## [6,] 4.058105 4.806485 那么MASS::mvrnorm()会返回一个矩阵，每个模拟的人对应一行，其中第一列表示对数身高，第二列表示对数体重。但是对数身高和对数体重对我们来说并不是很有用，所以让我们使用exp()函数来进行转换，它是log()转换的逆操作。 exp(log_ht_wt) ## height weight ## [1,] 70.40108 196.94276 ## [2,] 70.65632 133.64963 ## [3,] 41.36254 42.93844 ## [4,] 66.10779 117.24065 ## [5,] 114.43045 485.50576 ## [6,] 57.86453 122.30092 那么我们第一个模拟的人的身高是70.4英寸(大约是5’5”或者178.816 cm)，体重是196.94磅(约89.32 kg)。听起来感觉不错吧！(还是要注意，它会生成超出我们原始数据范围的观测值：我们会得到超高的人，例如第5个观测值，但至少体重/身高的关系会被保留)。 好的，让我们随机生成一群人的数据，将它们从对数转换为英寸和磅，然后将它们与我们的原始数据进行比较，看看效果如何。 ## 模拟新的人 new_humans &lt;- MASS::mvrnorm(500, c(height_in = 4.11, weight_lbs = 4.74), my_Sigma) %&gt;% exp() %&gt;% # 从对数转换回英寸和磅 as_tibble() %&gt;% # 为绘图转换为tibble格式 mutate(type = &quot;simulated&quot;) # 将他们标记为模拟(simulated) ## 合并真实和模拟的数据集，其中handw是来自heights_and_weights.csv的变量 alldata &lt;- bind_rows(handw %&gt;% mutate(type = &quot;real&quot;), new_humans) ggplot(alldata, aes(height_in, weight_lbs)) + geom_point(aes(colour = type), alpha = .1) 图2.7: 真实和模拟的人 你可以看到，我们模拟的人与正常的人非常相似，只是我们创建了一些身高和体重超出正常范围的人。 2.3 相关和回归的关系 好的，我们知道如何估计相关了，但如果我们想要根据身高来预测体重该怎么办呢？这可能听起来像一个不切实际的问题。但事实上，在使用或进行安全性取决于患者体重的药物或程序，但没时间称量患者体重时，急救医护人员可以使用这种技术，在紧急情况下迅速估算人们的体重。 回忆一下，简单回归模型的GLM是： \\[Y_i = \\beta_0 + \\beta_1 X_i + e_i.\\] 在这里，我们尝试根据他们观测到的身高(\\(X_i\\))来预测体重(\\(Y_i\\))。在这个方程里，\\(\\beta_0\\)和\\(\\beta_1\\)分别是y轴截距和斜率的参数，\\(e_i\\)是残差。传统上假设\\(e_i\\)的值来自均值为0、方差为\\(\\sigma^2\\)的正态分布；数学上的表述是\\(e_i \\sim N(0, \\sigma^2)\\)，其中\\(\\sim\\)表示“按照分布”，\\(N(0, \\sigma^2)\\)表示“均值为0、方差为\\(\\sigma^2\\)的正态分布(\\(N\\))”。 这表明如果我们有X和Y的均值估计值(分别标记为\\(\\mu_x\\)和\\(\\mu_y\\))、标准差估计值(\\(\\hat{\\sigma}_x\\)和\\(\\hat{\\sigma}_y\\))、X和Y之间相关系数的估计值(\\(\\hat{\\rho}\\))，我们就有了估计回归方程参数\\(\\beta_0\\)和\\(\\beta_1\\)所需要的所有信息。下面是具体做法。(译者注：这里的估计值是指根据样本信息来估计总体情况的估计值，是总体的估计值、样本的观测值) 首先，回归线的斜率\\(\\beta_1\\)等于相关系数\\(\\rho\\)乘以\\(Y\\)和\\(X\\)的标准差之比。 \\[\\beta_1 = \\rho \\frac{\\sigma_Y}{\\sigma_X}\\] 根据上面对数身高和对数体重的估计值，你能算出\\(\\beta_1\\)吗? b1 &lt;- .96 * (.65 / .26) b1 ## [1] 2.4 下一个要注意的点是，基于数学原理，回归线必须通过与\\(X\\)和\\(Y\\)均值对应的点，即点\\((\\mu_x, \\mu_y)\\)(你可以想象回归线根据斜率围绕该点“旋转”)。你也知道\\(\\beta_0\\)是y轴截距，即在\\(X = 0\\)处与纵轴相交的点。根据这些信息和上面的估计值，你能推断出\\(\\beta_0\\)的值吗？ 下面是你求解\\(\\beta_0\\)的推理过程。 想象一下，从\\(X = \\mu_x\\)逐步一个一个单位后退到\\(X = 0\\)。在\\(X = \\mu_x\\)处，\\(Y = 4.74\\)，每在X轴后退一个单位，\\(Y\\)将下降\\(\\beta_1 = 2.4\\)个单位。当你到0时，\\(Y\\)将从\\(\\mu_y\\)下降到\\(\\mu_y - \\mu_x \\beta_1\\)。 因此通解是：\\(\\beta_0 = \\mu_y - \\mu_x\\beta_1\\)。 因为\\(\\beta_1 = 2.4\\)、\\(\\mu_x = 4.11\\)、\\(\\mu_y = 4.74\\)，所以\\(\\beta_0 = -5.124\\)。因此，我们的回归方程是： \\[Y_i = -5.124 + 2.4X_i + e_i.\\] 为了验证我们的结果，我们先对对数转换后的数据进行回归，使用lm()函数，它采用最小二乘法(ordinary least squares regression)来估计参数。 summary(lm(wlog ~ hlog, handw_log)) ## ## Call: ## lm(formula = wlog ~ hlog, data = handw_log) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.63296 -0.09915 -0.01366 0.09285 0.65635 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -5.26977 0.13169 -40.02 &lt;2e-16 *** ## hlog 2.43304 0.03194 76.17 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1774 on 473 degrees of freedom ## Multiple R-squared: 0.9246, Adjusted R-squared: 0.9245 ## F-statistic: 5802 on 1 and 473 DF, p-value: &lt; 2.2e-16 看起来非常接近。不完全匹配的原因仅仅是我们将估计值四舍五入到小数点后两位以方便计算。 作为另一个检查，让我们将手动计算的回归线叠加在对数转换后数据的散点图上。 ggplot(handw_log, aes(hlog, wlog)) + geom_point(alpha = .2) + labs(x = &quot;log(height)&quot;, y = &quot;log(weight)&quot;) + geom_abline(intercept = -5.124, slope = 2.4, colour = &#39;blue&#39;) 图2.8: 对数值和叠加的回归线 看起来是对的。 最后，以下是相关性和回归之间关系的一些影响： 当\\(\\beta_1 = 0\\)时，与\\(\\rho = 0\\)相同。 当\\(\\beta_1 &gt; 0\\)时， \\(\\rho &gt; 0\\)，因为标准差不能为负。 当\\(\\beta_1 &lt; 0\\)时， \\(\\rho &lt; 0\\)，原因同上。 拒绝零假设\\(\\beta_1 = 0\\)与拒绝零假设\\(\\rho = 0\\)是相同的。在lm()中得到的\\(\\beta_1\\)的p值与使用cor.test()得到的\\(\\rho\\)的p值相同。 2.4 练习 "],["多元回归.html", "3 多元回归 3.1 一个例子：如何在统计学上取得好成绩 3.2 处理分类预测变量 3.3 多元回归和单因素方差分析(one-way ANOVA)的等价性 3.4 练习答案", " 3 多元回归 单层数据包含\\(m\\)个预测变量的一般模型是： \\[ Y_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\ldots + \\beta_m X_{mi} + e_i \\] 其中\\(e_i \\sim \\mathcal{N}\\left(0, \\sigma^2\\right)\\)，换句话说，我们假设误差来自一个均值为0，方差为\\(\\sigma^2\\)的正态分布。 注意，这里的关键假设不是响应变量(\\(Y\\))服从正态分布，也不是单个预测变量(\\(X\\))服从正态分布；而是仅模型残差服从正态分布(详细讨论见这篇博客)。单个\\(X\\)预测变量可以是任意组合的连续变量和/或分类变量，包括变量之间的交互。这个特定模型背后的进一步假设是，关系是”平面的”(可以用一个平面描述，类似于简单回归中的线性假设)，且误差方差与预测变量无关。 \\(\\beta\\)的值被称为回归系数(regression coefficient)。每个\\(\\beta_h\\)被解释为在保持其他所有预测变量不变的情况下\\(\\beta_h\\)的偏效应(partial effect)。如果你有\\(m\\)个预测变量，你将有\\(m+1\\)个回归系数：一个用于截距，每个预测变量各一个 尽管在统计教科书中对多元回归的讨论很常见，但你很少有机会应用到上述的精确模型。因为上述模型假设的是单层数据，而大多数心理学数据是多层的。然而，这两种数据集的基本原理是相同的，因此先学习比较简单的案例是值得的。 3.1 一个例子：如何在统计学上取得好成绩 让我们来看一些(虚构但基于现实的)数据，看看我们如何使用多元回归来回答各种研究问题。在这个假设的研究中，你有一个包含100名统计学学生的数据集，其中包括他们的最终课程成绩(grade)、每个学生参加讲座的次数(lecture，一个范围为0-10的整数)、每个学生点击下载在线资料的次数(nclicks)以及每个学生在修这门课程之前的平均绩点(GPA)，其范围从0(不及格)到4(最高可能成绩)。 3.1.1 数据导入和可视化 让我们加载数据grades.csv并看一看。 library(&quot;corrr&quot;) # 相关矩阵 library(&quot;tidyverse&quot;) grades &lt;- read_csv(&quot;data/grades.csv&quot;, col_types = &quot;ddii&quot;) grades ## # A tibble: 100 × 4 ## grade GPA lecture nclicks ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2.40 1.13 6 88 ## 2 3.67 0.971 6 96 ## 3 2.85 3.34 6 123 ## 4 1.36 2.76 9 99 ## 5 2.31 1.02 4 66 ## 6 2.58 0.841 8 99 ## 7 2.69 4 5 86 ## 8 3.05 2.29 7 118 ## 9 3.21 3.39 9 98 ## 10 2.24 3.27 10 115 ## # ℹ 90 more rows 首先，让我们看一看所有的两两相关。 grades %&gt;% correlate() %&gt;% shave() %&gt;% fashion() ## Correlation computed with ## • Method: &#39;pearson&#39; ## • Missing treated using: &#39;pairwise.complete.obs&#39; ## term grade GPA lecture nclicks ## 1 grade ## 2 GPA .25 ## 3 lecture .24 .44 ## 4 nclicks .16 .30 .36 pairs(grades) 图2.2: grades数据集中的所有成对关系 3.1.2 估计和解释 我们将使用lm()函数来估计回归系数(\\(\\beta\\)s)。针对一个有\\(m\\)个预测变量的GLM： \\[ Y_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\ldots + \\beta_m X_{mi} + e_i \\] 对base R的lm()调用如下： lm(Y ~ X1 + X2 + ... + Xm, data) 变量Y是你的响应变量，变量X是预测变量。注意，你不需要明确指定截距或残差项;这些是默认包含的。 对于当前数据，让我们通过lecture和nclicks来预测grade。 my_model &lt;- lm(grade ~ lecture + nclicks, grades) summary(my_model) ## ## Call: ## lm(formula = grade ~ lecture + nclicks, data = grades) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.21653 -0.40603 0.02267 0.60720 1.38558 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.462037 0.571124 2.560 0.0120 * ## lecture 0.091501 0.045766 1.999 0.0484 * ## nclicks 0.005052 0.006051 0.835 0.4058 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8692 on 97 degrees of freedom ## Multiple R-squared: 0.06543, Adjusted R-squared: 0.04616 ## F-statistic: 3.395 on 2 and 97 DF, p-value: 0.03756 我们通常会在参数符号的顶部加上一个帽子(hat)，以明确我们正在处理样本的估计值，而不是(未知的)真实总体值。由上可知: \\(\\hat{\\beta}_0\\) = 1.46 \\(\\hat{\\beta}_1\\) = 0.09 \\(\\hat{\\beta}_2\\) = 0.01 这告诉我们，一个人的预期成绩与他们的参加讲座的次数和点击下载在线资料的次数通过以下公式相关联： grade = 1.46 + 0.09 \\(\\times\\) lecture + 0.01 \\(\\times\\) nclicks 因为\\(\\hat{\\beta}_1\\)和\\(\\hat{\\beta}_2\\)都是正数，所以我们知道lecture和nclicks的值越高，成绩越好。 因此，如果有人问你，你预测一个参加了3次讲座并下载了70次的学生成绩是多少，你可以通过代入相应的值轻松算出来。 grade = 1.46 + 0.09 \\(\\times\\) 3 + 0.01 \\(\\times\\) 70 相当于： grade = 1.46 + 0.27 + 0.7 可化简为： grade = 2.43 3.1.3 使用predict()通过线性模型进行预测 如果我们想通过新的预测变量值来预测响应变量值，我们可以使用base R 的predict()函数。 predict()函数有两个主要参数。第一个参数是拟合的模型对象(即上面的my_model)，第二个参数是包含预测变量新值的数据框(或tibble，R中和数据框data frame类似的一种数据格式)。 在新表中，你需要包含所有的预测变量。如果您的tibble缺少任何预测变量，你将收到错误消息(error message)。您还需要确保新表中的变量名与模型中的变量名完全匹配。 让我们创建一个带有新值的tibble并进行测试。 ## “tribble”是一种按行而不是按列创建tibble的方法。有时这样做会很有用。 new_data &lt;- tribble(~lecture, ~nclicks, 3, 70, 10, 130, 0, 20, 5, 100) tribble()函数提供了一种按行而不是按列逐步构建tibble的方法，而使用tibble()函数则是按列逐步构建表格。 tribble()的第一行包含列名，每个列名前面都有一个波浪号(~)。 有时这种方法比逐行构建更容易阅读，尽管结果是相同的。考虑到这些，我们也可以使用以下方式创建上述表格： new_data &lt;- tibble(lecture = c(3, 10, 0, 5), nclicks = c(70, 130, 20, 100)) 现在我们已经创建了表new_data，只需将其传递给predict()函数，这会返回一个关于\\(Y\\)(grade)预测值的向量。 predict(my_model, new_data) ## 1 2 3 4 ## 2.090214 3.033869 1.563087 2.424790 这很好，但也许我们希望将预测值和预测变量对应起来。我们可以通过将预测值作为新列添加到new_data中来实现这一点。 new_data %&gt;% mutate(predicted_grade = predict(my_model, new_data)) ## # A tibble: 4 × 3 ## lecture nclicks predicted_grade ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3 70 2.09 ## 2 10 130 3.03 ## 3 0 20 1.56 ## 4 5 100 2.42 想查看predict()的更多操作吗？使用?predict.lm来获得帮助。 3.1.4 偏效应可视化 如上所述，每个回归系数的参数估计值告诉我们该变量的偏效应；即保持其他所有变量不变时它的效应。有办法可视化这个偏效应吗？是的，你可以使用predict()函数来实现，通过创建一个表，其中焦点预测变量(focal predictor)的值是多样的，其他所有预测变量用均值填充。 例如，我们要可视化lecture对grade的偏效应，同时将nclicks的值保持在其均值不变。 nclicks_mean &lt;- grades %&gt;% pull(nclicks) %&gt;% mean() ## 预测用的新数据 new_lecture &lt;- tibble(lecture = 0:10, nclicks = nclicks_mean) ## 将预测值添加到new_lecture new_lecture2 &lt;- new_lecture %&gt;% mutate(grade = predict(my_model, new_lecture)) new_lecture2 ## # A tibble: 11 × 3 ## lecture nclicks grade ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 98.3 1.96 ## 2 1 98.3 2.05 ## 3 2 98.3 2.14 ## 4 3 98.3 2.23 ## 5 4 98.3 2.32 ## 6 5 98.3 2.42 ## 7 6 98.3 2.51 ## 8 7 98.3 2.60 ## 9 8 98.3 2.69 ## 10 9 98.3 2.78 ## 11 10 98.3 2.87 现在让我们作图。 ggplot(grades, aes(lecture, grade)) + geom_point() + geom_line(data = new_lecture2) 图3.1: nclicks固定在均值，lecture对成绩的偏效应 偏效应图只有在模型中焦点预测变量与其他预测变量之间没有交互作用时才有意义。 原因是当存在交互作用时，焦点预测变量\\(X_i\\)的偏效应会随着与其交互的变量的变动而变动。 现在你能可视化nclicks对grade的偏效应吗? 本页最后给出解决方法。 3.1.5 标准化系数 我们经常用多元回归来解决的一类问题是，哪个预测变量对预测Y最重要？ 现在，你不能简单地读取\\(\\hat{\\beta}\\)值并选择绝对值最大的一个，因为这些预测变量都在不同的尺度上。为了回答这个问题，你需要对预测变量进行中心化(center)和比例化(scale)。 还记得\\(z\\)分数吗？ \\[ z = \\frac{X - \\mu_x}{\\sigma_x} \\] \\(z\\)分数(\\(z\\)-score)表示\\(X\\)与样本均值(\\(\\mu_x\\))之间的标准差单位距离(\\(\\sigma_x\\))。因此，\\(z\\)分数为1意味着该值高于均值1个标准差；\\(z\\)分数为-2.5意味着低于均值2.5个标准差。\\(Z\\)分数通过将它们校准为标准正态分布(均值为0，标准差为1的分布)给了我们一种比较来自不同总体的事物的办法。 那么我们通过将预测变量转换为\\(z\\)分数来重新缩放它们。这是相当容易做到的。 grades2 &lt;- grades %&gt;% mutate(lecture_c = (lecture - mean(lecture)) / sd(lecture), nclicks_c = (nclicks - mean(nclicks)) / sd(nclicks)) grades2 ## # A tibble: 100 × 6 ## grade GPA lecture nclicks lecture_c nclicks_c ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2.40 1.13 6 88 -0.484 -0.666 ## 2 3.67 0.971 6 96 -0.484 -0.150 ## 3 2.85 3.34 6 123 -0.484 1.59 ## 4 1.36 2.76 9 99 0.982 0.0439 ## 5 2.31 1.02 4 66 -1.46 -2.09 ## 6 2.58 0.841 8 99 0.493 0.0439 ## 7 2.69 4 5 86 -0.972 -0.796 ## 8 3.05 2.29 7 118 0.00488 1.27 ## 9 3.21 3.39 9 98 0.982 -0.0207 ## 10 2.24 3.27 10 115 1.47 1.08 ## # ℹ 90 more rows 现在让我们用居中和缩放后的预测变量重新拟合模型。 my_model_scaled &lt;- lm(grade ~ lecture_c + nclicks_c, grades2) summary(my_model_scaled) ## ## Call: ## lm(formula = grade ~ lecture_c + nclicks_c, data = grades2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.21653 -0.40603 0.02267 0.60720 1.38558 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.59839 0.08692 29.895 &lt;2e-16 *** ## lecture_c 0.18734 0.09370 1.999 0.0484 * ## nclicks_c 0.07823 0.09370 0.835 0.4058 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8692 on 97 degrees of freedom ## Multiple R-squared: 0.06543, Adjusted R-squared: 0.04616 ## F-statistic: 3.395 on 2 and 97 DF, p-value: 0.03756 这告诉我们lecture_c的影响相对更大，该变量每个标准差的提高，grade相应地提高0.19。 另一种常见的标准化方法是同时对响应变量和预测变量进行标准化，即对\\(Y\\)值和\\(X\\)值都进行\\(z\\)分数转换。采用这种方法时，回归系数的相对(影响力)排序将保持不变。主要区别在于，系数将反映响应变量以标准差(\\(SD\\))为单位的变化，而不是原始单位。 多重共线性(multicollinearity)及其不足 在关于多元回归的讨论中，你可能会听到对”多重共线性”的担忧，这是指预测变量之间存在交互关系的一种高级说法。这只是一个潜在的问题，因为它可能会影响对单个预测变量效应的解释。当预测变量相关时，\\(\\beta\\)值可能会根据模型中包含或排除的预测变量而变化，有时甚至会改变符号。关于这一点需要记住的关键是： 观察性研究(observational studies)中预测变量之间相关是不可避免的； 回归不假设你的预测变量彼此独立(换句话说，在预测变量之间找到相关性本身并不是质疑模型的理由)； 当存在强相关时，解释单个回归系数时要谨慎； 目前没有已知的”补救措施”，也不清楚是否需要任何此类补救措施，许多所谓的补救措施弊大于利。 更多信息和指导请查阅(Vanhove, 2021)。 3.1.6 模型比较 另一种使用多元回归模型解决的常见问题是：某个预测变量或一组预测变量在超出某些控制变量的影响之外，对我的响应变量有显著影响吗？ 举个例子，上述包含lecture和nclicks的模型在统计上是显著的， \\(F(2,97) = 3.395, p =0.038\\)。 \\(m\\)个预测因子的回归模型的原假设(null hypothesis)为： \\[H_0: \\beta_1 = \\beta_2 = \\ldots = \\beta_m = 0;\\] 换句话说，所有的系数(除了截距)都是0。如果原假设成立，那么原模型(null model) \\[Y_i = \\beta_0\\] 与包含所有预测变量及其系数的模型一样能给出很好的预测。换句话说，你对\\(Y\\)的最佳预测只是它的均值(\\(\\mu_y\\))；\\(X\\)变量是无关紧要的。我们拒绝了这个原假设，意味着通过纳入我们的两个预测变量lecture和nclicks，可以做得更好。 但你可能会问：也许是成绩更好的学生获得了更好的成绩，而lecture、nclicks和grade之间的关系仅仅是通过学生质量的中介来实现。毕竟，成绩更好的学生更有可能去听讲座并下载材料。因此，我们可以问，出勤率和下载次数是否在超出学生能力(通过GPA测量)之外与更好的成绩相关？ 我们可以通过模型比较来检验这一假设。逻辑是这样的：首先，估计一个包含所有控制预测变量但不包含焦点预测变量的模型。其次，估计一个包含控制预测变量和焦点预测变量的模型。最后，比较这两个模型，看看包含预测因子是否有统计学上显著的增益。 下面展示你如何做到这一点： m1 &lt;- lm(grade ~ GPA, grades) # control model m2 &lt;- lm(grade ~ GPA + lecture + nclicks, grades) # bigger model anova(m1, m2) ## Analysis of Variance Table ## ## Model 1: grade ~ GPA ## Model 2: grade ~ GPA + lecture + nclicks ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 98 73.528 ## 2 96 71.578 2 1.9499 1.3076 0.2752 原假设是我们仅通过GPA预测grade与通过GPA加上lecture和nclicks预测grade一样准确。如果添加这两个变量能够显著减少残差平方和(RSS)，即它们能解释足够多的残差方差，我们就会拒绝这个原假设。 我们看到情况并非如此：\\(F(2,96) = 1.308\\), \\(p =0.275\\)。因此，我们没有证据表明课堂出勤率和下载在线材料在通过GPA衡量的学生能力之外与更好的成绩相关。 3.2 处理分类预测变量 回归公式将响应变量描述为加权预测变量的总和。但如果其中一个预测变量是分类变量(如代表”农村”或”城市”等群组)，而不是数值型变量会怎样呢？许多变量是名义(nominal)变量：包含名称的分类变量，在变量的级别之间没有固有的顺序。例如，宠物所有权(猫、狗、雪貂)是一个名义变量；不考虑喜好，拥有猫不等于拥有狗，拥有狗不等于拥有雪貂。 使用数值型预测变量表示名义数据 在回归模型中表示一个\\(k\\)水平的名义变量需要\\(k-1\\)个数值型预测变量；例如，如果有4水平，你需要3个预测变量。大多数编码方案要求你在\\(k\\)个水平中选择一个作为基线水平。\\(k-1\\)个变量中，每个变量都是其表示的水平与基线水平对比。 举例：你有一个三水平变量pet_type(猫、狗、雪貂)。 你选择猫为基线并创建两个数值型预测变量： dog_v_cat用来编码狗和猫之间的对比； ferret_v_cat用来编码雪貂和猫之间的对比。 名义变量通常在数据框中表示character或factor类型。 字符(character)变量和因子(factor)变量的区别在于因子包含关于水平及其顺序的信息，而字符向量则缺乏此信息。 当你使用R公式语法(R formula syntax)指定模型时，R会检查公式右侧预测变量的数据类型。例如，如果你的模型将income回归到pet_type上(income ~ pet_type)，R会检查pet_type的数据类型。 对于所有类型为字符或因子的变量，R都会隐式地(implicitly)创建一个(或一组)数值型预测变量来表示该变量在模型中的作用。有多种方案可用于创建名义变量的数值表示。R中的默认方法是使用虚拟(dummy，或叫处理(treatment))编码(见下文)。不幸的是，这种默认方法(注：不是指虚拟编码，而是指R自动建立虚拟编码)不适合心理学中的许多研究设计，因此我建议你学习如何“手动”编码你的预测变量，并养成这样做的习惯。 不要用数字表示分类变量的水平！ 上述例子中，我们有一个名为pet_type的变量，其水平为cat、dog和ferret。有时人们用数字表示名义变量的水平，像这样： 1表示猫， 2表示狗， 3表示雪貂。 这是个坏主意。 首先，这样的标记是任意且不透明的，任何试图使用你数据的人都不知道哪个数字对应哪个类别(你自己也可能会忘记！)。 更糟糕的是，如果你把这个变量作为回归模型中的预测变量引入，R将无法知道你使用1、2和3作为变量内各水平标签的意图，而是会假设pet_type是一个测量值，其中狗比猫大1个单位，雪貂比猫大2个单位，比狗大1个单位，这完全是无意义的！ 这种错误太容易犯了，而且如果作者不分享他们的数据和代码，这很难发现。2016年，《Current Biology》发表的一篇关于儿童宗教信仰和利他行为的论文就因为这种错误而被撤稿。 所以，不要用数字来表示名义变量的各个水平，除非你是有意创建预测变量来编码代表名义变量的\\(k-1\\)对比项，以便在回归模型中正确表示名义变量。 3.2.1 虚拟编码(又称“处理”编码) 对于只有两个水平的名义变量，选择一个水平作为基线，并创建一个新变量，当名义变量是基线时，该新变量为0，非基线时为1。基线的选择是任意的，只会影响系数是正还是负，但不会影响大小、标准误或相关的p值。 为了说明这点，让我们生成一些只包含一个两水平分类预测变量的假数据。 fake_data &lt;- tibble(Y = rnorm(10), group = rep(c(&quot;A&quot;, &quot;B&quot;), each = 5)) fake_data ## # A tibble: 10 × 2 ## Y group ## &lt;dbl&gt; &lt;chr&gt; ## 1 -1.63 A ## 2 -0.776 A ## 3 0.332 A ## 4 -0.798 A ## 5 1.26 A ## 6 -0.676 B ## 7 -0.476 B ## 8 0.568 B ## 9 -0.464 B ## 10 0.787 B 现在让我们来添加一个新变量group_d，这是经过虚拟编码的分组变量。我们将使用dplyr::if_else()函数来定义新列。 fake_data2 &lt;- fake_data %&gt;% mutate(group_d = if_else(group == &quot;B&quot;, 1, 0)) fake_data2 ## # A tibble: 10 × 3 ## Y group group_d ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 -1.63 A 0 ## 2 -0.776 A 0 ## 3 0.332 A 0 ## 4 -0.798 A 0 ## 5 1.26 A 0 ## 6 -0.676 B 1 ## 7 -0.476 B 1 ## 8 0.568 B 1 ## 9 -0.464 B 1 ## 10 0.787 B 1 现在我们把它作为常规的回归模型来运行。 summary(lm(Y ~ group_d, fake_data2)) ## ## Call: ## lm(formula = Y ~ group_d, data = fake_data2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.3120 -0.4698 -0.4177 0.6465 1.5849 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.3230 0.4160 -0.776 0.460 ## group_d 0.2706 0.5882 0.460 0.658 ## ## Residual standard error: 0.9301 on 8 degrees of freedom ## Multiple R-squared: 0.02577, Adjusted R-squared: -0.096 ## F-statistic: 0.2117 on 1 and 8 DF, p-value: 0.6577 让我们反转编码。我们得到相同的结果，只是符号不同。 fake_data3 &lt;- fake_data %&gt;% mutate(group_d = if_else(group == &quot;A&quot;, 1, 0)) summary(lm(Y ~ group_d, fake_data3)) ## ## Call: ## lm(formula = Y ~ group_d, data = fake_data3) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.3120 -0.4698 -0.4177 0.6465 1.5849 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.05235 0.41595 -0.126 0.903 ## group_d -0.27063 0.58824 -0.460 0.658 ## ## Residual standard error: 0.9301 on 8 degrees of freedom ## Multiple R-squared: 0.02577, Adjusted R-squared: -0.096 ## F-statistic: 0.2117 on 1 and 8 DF, p-value: 0.6577 截距的解释是编码为0的组的估计均值。你可以通过将0代入到下面的预测公式中的X看到这一点。因此，\\(\\beta_1\\)可以解释为基线组和编码为1的组之间的均值差异。 \\[\\hat{Y_i} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i \\] 请注意，如果我们只是将字符变量group作为模型的预测变量，R将根据需要自动为我们创建虚拟变量。 lm(Y ~ group, fake_data) %&gt;% summary() ## ## Call: ## lm(formula = Y ~ group, data = fake_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.3120 -0.4698 -0.4177 0.6465 1.5849 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.3230 0.4160 -0.776 0.460 ## groupB 0.2706 0.5882 0.460 0.658 ## ## Residual standard error: 0.9301 on 8 degrees of freedom ## Multiple R-squared: 0.02577, Adjusted R-squared: -0.096 ## F-statistic: 0.2117 on 1 and 8 DF, p-value: 0.6577 lm()函数检查group并确定变量不同的水平——在这种情况下是A和B。然后，它选择按字母顺序排在最前面的水平作为基线(注：中文一样是字母顺序)，并编码另一个水平(B)与基线水平(A)之间的对比。(如果group被定义为因子，基线水平就是levels(fake_data$group)的第一个元素)。 它创建的新变量以groupB的名称出现在输出中。 3.2.2 当\\(k &gt; 2\\)时的虚拟编码 当名义预测变量超过两水平(\\(k &gt; 2\\))时，一个数值预测变量就不够用了，我们需要\\(k-1\\)个预测变量。如果名义预测变量有4水平，我们将需要定义3个预测变量。让我们模拟一些要处理的数据，season_wt表示一年四季一个人的体重(以kg为单位)。 season_wt &lt;- tibble(season = rep(c(&quot;winter&quot;, &quot;spring&quot;, &quot;summer&quot;, &quot;fall&quot;), each = 5), bodyweight_kg = c(rnorm(5, 105, 3), rnorm(5, 103, 3), rnorm(5, 101, 3), rnorm(5, 102.5, 3))) season_wt ## # A tibble: 20 × 2 ## season bodyweight_kg ## &lt;chr&gt; &lt;dbl&gt; ## 1 winter 107. ## 2 winter 106. ## 3 winter 99.4 ## 4 winter 99.4 ## 5 winter 99.7 ## 6 spring 103. ## 7 spring 105. ## 8 spring 103. ## 9 spring 104. ## 10 spring 104. ## 11 summer 98.7 ## 12 summer 104. ## 13 summer 104. ## 14 summer 100. ## 15 summer 103. ## 16 fall 101. ## 17 fall 99.0 ## 18 fall 98.8 ## 19 fall 97.4 ## 20 fall 102. 现在让我们添加三个预测变量来编码变量season。尝试一下，看看你是否能弄清楚了它是如何工作的。 ## 基线变量是“winter” season_wt2 &lt;- season_wt %&gt;% mutate(spring_v_winter = if_else(season == &quot;spring&quot;, 1, 0), summer_v_winter = if_else(season == &quot;summer&quot;, 1, 0), fall_v_winter = if_else(season == &quot;fall&quot;, 1, 0)) season_wt2 ## # A tibble: 20 × 5 ## season bodyweight_kg spring_v_winter summer_v_winter fall_v_winter ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 winter 107. 0 0 0 ## 2 winter 106. 0 0 0 ## 3 winter 99.4 0 0 0 ## 4 winter 99.4 0 0 0 ## 5 winter 99.7 0 0 0 ## 6 spring 103. 1 0 0 ## 7 spring 105. 1 0 0 ## 8 spring 103. 1 0 0 ## 9 spring 104. 1 0 0 ## 10 spring 104. 1 0 0 ## 11 summer 98.7 0 1 0 ## 12 summer 104. 0 1 0 ## 13 summer 104. 0 1 0 ## 14 summer 100. 0 1 0 ## 15 summer 103. 0 1 0 ## 16 fall 101. 0 0 1 ## 17 fall 99.0 0 0 1 ## 18 fall 98.8 0 0 1 ## 19 fall 97.4 0 0 1 ## 20 fall 102. 0 0 1 提醒：始终查看你的数据 每当你写的代码可能改变数据时，都应该通过查看数据来确保代码按预期工作。这在你手动编码用于回归的名义变量时尤其重要，因为有时代码会写错，但不会引发报错。 考虑上面的代码块，我们定义了三个对比来表示名义变量season，winter是我们的基线。 如果你意外地拼错了一个水平(使用summre而不是summer)，你能注意到吗？ season_wt3 &lt;- season_wt %&gt;% mutate(spring_v_winter = if_else(season == &quot;spring&quot;, 1, 0), summer_v_winter = if_else(season == &quot;summre&quot;, 1, 0), fall_v_winter = if_else(season == &quot;fall&quot;, 1, 0)) 虽然上面的代码块可以运行，但当我们运行回归时，我们会得到令人困惑的输出，即summer_v_winter的系数是NA(不可用，not available)。 lm(bodyweight_kg ~ spring_v_winter + summer_v_winter + fall_v_winter, season_wt3) ## ## Call: ## lm(formula = bodyweight_kg ~ spring_v_winter + summer_v_winter + ## fall_v_winter, data = season_wt3) ## ## Coefficients: ## (Intercept) spring_v_winter summer_v_winter fall_v_winter ## 102.009 1.850 NA -2.288 发生了什么？让我们看看数据来找到答案。我们将使用distinct函数找出原始变量season和我们创建的三个变量的不同组合(详细信息参阅?dplyr::distinct)。 season_wt3 %&gt;% distinct(season, spring_v_winter, summer_v_winter, fall_v_winter) ## # A tibble: 4 × 4 ## season spring_v_winter summer_v_winter fall_v_winter ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 winter 0 0 0 ## 2 spring 1 0 0 ## 3 summer 0 0 0 ## 4 fall 0 0 1 由于我们的拼写错误，当season == \"summer\"时，预测变量summer_v_winter不是1；相反，它总是0。if_else()字面上意思是“如果season == \"summre\"，则将summer_v_winter设置为1，否则为0”。当然，season永远不会等于summre，因为summre是一个拼写错误。我们本可以通过使用distinct()进行上面的检查轻松发现这一点。当你创建自己的数值预测变量时，养成这样做的习惯是很重要的。 更仔细地看看R的默认情况 如果你曾经使用过像SPSS这样的指向-点击的统计软件，你可能从未学习过如何编码分类预测变量。通常，软件会识别预测变量是否为分类变量，并在后台将其重新编码为数值型预测变量。R也是如此：如果你将character或factor类型的预测变量提供给线性建模函数，它将为你创建数值型虚拟编码的预测变量，如下方代码所示。 lm(bodyweight_kg ~ season, season_wt) %&gt;% summary() ## ## Call: ## lm(formula = bodyweight_kg ~ season, data = season_wt) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.9684 -1.7978 -0.3415 1.4710 4.8183 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 99.721 1.105 90.236 &lt;2e-16 *** ## seasonspring 4.138 1.563 2.648 0.0176 * ## seasonsummer 1.983 1.563 1.269 0.2227 ## seasonwinter 2.594 1.563 1.660 0.1164 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.471 on 16 degrees of freedom ## Multiple R-squared: 0.3104, Adjusted R-squared: 0.1811 ## F-statistic: 2.401 on 3 and 16 DF, p-value: 0.1058 在这里，R隐式地创建了三个虚拟变量来编码season的四个水平，分别命名为seasonspring、seasonsummer和seasonwinter。未提及的季节fall被选为基线，因为它在字母表中最早出现。这三个预测变量的值如下： seasonspring：如果是春季则为1，否则为0； seasonsummer：如果是夏季则为1，否则为0； seasonwinter：如果是冬季则为1，否则为0； 这似乎是让R为我们做的一件方便的事情，但是在按赖默认设置时可能会有潜在风险。在下一章中，当我们讨论交互作用时，我们将更多地了解这些风险。 3.3 多元回归和单因素方差分析(one-way ANOVA)的等价性 如果我们想要查看我们的体重是否随季节变化，我们可以对season_wt2进行单因素方差分析，如下所示。 ## 将季节变为一个以“winter”为基线的因子变量 season_wt3 &lt;- season_wt2 %&gt;% mutate(season = factor(season, levels = c(&quot;winter&quot;, &quot;spring&quot;, &quot;summer&quot;, &quot;fall&quot;))) my_anova &lt;- aov(bodyweight_kg ~ season, season_wt3) summary(my_anova) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## season 3 43.98 14.660 2.401 0.106 ## Residuals 16 97.70 6.106 好了，现在我们可以用下面的回归模型复制这个结果吗? \\[Y_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{3i} + e_i\\] summary(lm(bodyweight_kg ~ spring_v_winter + summer_v_winter + fall_v_winter, season_wt2)) ## ## Call: ## lm(formula = bodyweight_kg ~ spring_v_winter + summer_v_winter + ## fall_v_winter, data = season_wt2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.9684 -1.7978 -0.3415 1.4710 4.8183 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 102.3146 1.1051 92.583 &lt;2e-16 *** ## spring_v_winter 1.5438 1.5629 0.988 0.338 ## summer_v_winter -0.6114 1.5629 -0.391 0.701 ## fall_v_winter -2.5940 1.5629 -1.660 0.116 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.471 on 16 degrees of freedom ## Multiple R-squared: 0.3104, Adjusted R-squared: 0.1811 ## F-statistic: 2.401 on 3 and 16 DF, p-value: 0.1058 请注意，这两种方法的 \\(F\\) 值和 \\(p\\) 值是相同的! 3.4 练习答案 展开答案 首先创建一个包含新预测变量的tibble。我们可能还想知道nclicks的取值范围。 lecture_mean &lt;- grades %&gt;% pull(lecture) %&gt;% mean() min_nclicks &lt;- grades %&gt;% pull(nclicks) %&gt;% min() max_nclicks &lt;- grades %&gt;% pull(nclicks) %&gt;% max() ## 预测变量的新数据 new_nclicks &lt;- tibble(lecture = lecture_mean, nclicks = min_nclicks:max_nclicks) ## 将预测值添加到new_lecture中 new_nclicks2 &lt;- new_nclicks %&gt;% mutate(grade = predict(my_model, new_nclicks)) new_nclicks2 ## # A tibble: 76 × 3 ## lecture nclicks grade ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 6.99 54 2.37 ## 2 6.99 55 2.38 ## 3 6.99 56 2.38 ## 4 6.99 57 2.39 ## 5 6.99 58 2.39 ## 6 6.99 59 2.40 ## 7 6.99 60 2.40 ## 8 6.99 61 2.41 ## 9 6.99 62 2.41 ## 10 6.99 63 2.42 ## # ℹ 66 more rows 现在作图。 ggplot(grades, aes(nclicks, grade)) + geom_point() + geom_line(data = new_nclicks2) 图3.2: nclicks对grade的偏效应图 参考文献 Vanhove, J. (2021). Collinearity isn’t a disease that needs curing. Meta-Psychology, 5. "],["交互作用.html", "4 交互作用 4.1 连续变量-分类变量交互作用 4.2 分类变量-分类变量交互作用", " 4 交互作用 到目前为止，我们一直专注于估计和解释一个变量或预测变量线性组合对响应变量的影响。然而，往往存在这样的情况，一个预测变量对响应变量的影响取决于另一个预测变量。实际上，我们可以在模型中包含一个交互项来估计和解释这种依赖性。 4.1 连续变量-分类变量交互作用 让我们考虑一个简单的虚构例子。假设你对声波干扰对认知表现的影响感兴趣。你的研究中的每个被试在执行一个简单的反应时任务时(尽快对闪光灯做出反应)，被随机分配接受特定水平的声波干扰。你有一种技术，可以自动生成不同水平的背景噪音（例如城市声音的频率和振幅：鸣笛声、钻地声、喧哗声、玻璃破碎声等）。每个参与者在一个随机选择的干扰水平(0到100)下执行任务。你的假设是，城市生活使人们的任务表现更不受声波干扰的影响。你想要比较城市居民和乡村居民(来自更安静的乡村环境)之间干扰与表现关系的差异。 你有3个变量： 一个连续响应变量，mean_RT，其较高的水平反映较慢的反应时； 一个连续预测变量，声波干扰水平(dist_level)，其较高的水平表示更多的干扰； 一个两水平的因子，group (城市vs.乡村)。 让我们从模拟一些城市组的数据开始。假设在0干扰(静音)下，平均反应时约为450毫秒，且干扰每增加1个单位，反应时就增加约2毫秒。这给了我们以下线性模型： \\[Y_i = 450 + 2 X_i + e_i\\] 其中\\(X_i\\)是声音干扰的水平。 让我们模拟100名被试的数据，设定\\(\\sigma = 80\\)，并在开始之前设置种子。 library(tidyverse) set.seed(1031) n_subj &lt;- 100L # 模拟100名被试的数据(注：L是说明100是整数) b0_urban &lt;- 450 # y轴截距 b1_urban &lt;- 2 # 斜率 # 分解表(decomposition table) urban &lt;- tibble( subj_id = 1:n_subj, group = &quot;urban&quot;, b0 = 450, b1 = 2, dist_level = sample(0:n_subj, n_subj, replace = TRUE), err = rnorm(n_subj, mean = 0, sd = 80), simple_rt = b0 + b1 * dist_level + err) urban ## # A tibble: 100 × 7 ## subj_id group b0 b1 dist_level err simple_rt ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 urban 450 2 59 -36.1 532. ## 2 2 urban 450 2 45 128. 668. ## 3 3 urban 450 2 55 23.5 584. ## 4 4 urban 450 2 8 1.04 467. ## 5 5 urban 450 2 47 48.7 593. ## 6 6 urban 450 2 96 88.2 730. ## 7 7 urban 450 2 62 110. 684. ## 8 8 urban 450 2 8 -91.6 374. ## 9 9 urban 450 2 15 -109. 371. ## 10 10 urban 450 2 70 20.7 611. ## # ℹ 90 more rows 让我们绘制创建的数据，并作出最佳拟合线。 ggplot(urban, aes(dist_level, simple_rt)) + geom_point(alpha = .2) + geom_smooth(method = &quot;lm&quot;, se = FALSE) 图4.1: 声波干扰对简单反应时的影响(城市组) 现在让我们为乡村组模拟数据。我们假设这些被试的截距可能会略高一些，可能是因为他们对技术不太熟悉。最重要的是，我们假设他们的斜率会更陡，因为他们受到噪音的影响更大。大致如下： \\[Y_i = 500 + 3 X_i + e_i\\] b0_rural &lt;- 500 b1_rural &lt;- 3 rural &lt;- tibble( subj_id = 1:n_subj + n_subj, group = &quot;rural&quot;, b0 = b0_rural, b1 = b1_rural, dist_level = sample(0:n_subj, n_subj, replace = TRUE), err = rnorm(n_subj, mean = 0, sd = 80), simple_rt = b0 + b1 * dist_level + err) 现在让我们把这两组的数据一起画出来。 all_data &lt;- bind_rows(urban, rural) ggplot(all_data %&gt;% mutate(group = fct_relevel(group, &quot;urban&quot;)), aes(dist_level, simple_rt, colour = group)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + facet_wrap(~ group) + theme(legend.position = &quot;none&quot;) 图4.2: 声波干扰对简单反应时的影响(城市和乡村) 这里我们可以很清楚地看到我们在数据中建立的斜率差异。我们如何测试两个斜率是否有显著不同呢？要做到这一点，我们不能做两个单独的回归。我们需要将这两条回归线纳入同一个模型中。我们应该怎么做呢？ 请注意，我们可以用”偏移(offset)“值来表示其中一条回归线。我们(任意地)选一组作为我们的”基线”组，并将另一组的y轴截距和斜率表示为相对于这个基准的偏移值。因此，如果我们选择城市组作为基线，我们可以用两个偏移值\\(\\beta_2\\)和\\(\\beta_3\\)分别表示乡村组的y轴截距和斜率偏移。 y轴截距: \\(\\beta_{0\\_rural} = \\beta_{0\\_urban} + \\beta_2\\) 斜率: \\(\\beta_{1\\_rural} = \\beta_{1\\_urban} + \\beta_3\\) 我们有城市组参数：\\(\\beta_{0\\_urban} = 450\\)和\\(\\beta_{1\\_urban} = 2\\)，乡村组参数：\\(\\beta_{0\\_rural} = 500\\)和\\(\\beta_{1\\_rural} = 3\\)。因此可以得出： \\(\\beta_2 = 50\\)，因为\\(\\beta_{0\\_rural} - \\beta_{0\\_urban} = 500 - 450 = 50\\) \\(\\beta_3 = 1\\)，因为\\(\\beta_{1\\_rural} - \\beta_{1\\_urban} = 3 - 2 = 1\\) 现在我们的两个回归模型是： \\[Y_{i\\_urban} = \\beta_{0\\_urban} + \\beta_{1\\_urban} X_i + e_i\\] 和 \\[Y_{i\\_rural} = (\\beta_{0\\_urban} + \\beta_2) + (\\beta_{1\\_urban} + \\beta_3) X_i + e_i\\] 好的，看起来我们更接近将这些模型合并为一个单一的回归模型了。这里有最后的技巧。我们定义一个额外的虚拟变量，该变量在城市组中取值为0(我们选择作为”基线”组)，在其他组中取值为1。下框包含我们的最终模型。 含有连续-分类变量交互作用的回归模型 \\[Y_{i} = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i} X_{2i} + e_{i}\\] 其中 \\(X_{1i}\\)是连续变量 \\(X_{2i}\\)是虚拟变量，0表示基线组，1表示其他组 参数解释： \\(\\beta_0\\): 基线组y轴截距； \\(\\beta_1\\): 基线组斜率； \\(\\beta_2\\): 其他组的y轴截距偏移量； \\(\\beta_3\\): 其他组的斜率偏移量。 用R估计： lm(Y ~ X1 + X2 + X1:X2, data)或者缩写: lm(Y ~ X1 * X2, data)。这里*的意思是：“所有可能的主效应以及X1和X2的交互” \\(\\beta_3 X_{1i} X_{2i}\\)项是两个预测变量相乘，被称为交互项(interaction term)。现在让我们展示上述广义线性模型(GLM)是如何得出两条回归线的。 为了得到城市组的回归方程，我们将0代入到\\(X_{2i}\\)中。得到公式： \\[Y_{i} = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 0 + \\beta_3 X_{1i} 0 + e_i\\] 去掉等于0的项，就得到： \\[Y_{i} = \\beta_0 + \\beta_1 X_{1i} + e_i,\\] 这就是基线组(城市组)的回归方程。将其与上面的\\(Y_{i\\_urban}\\)进行对比。 将1代入到\\(X_{2i}\\)里会得到乡村组的方程。我们得到： \\[Y_{i} = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 1 + \\beta_3 X_{1i} 1 + e_i\\] 化简和运用一些代数后，也能表示为： \\[Y_{i} = \\beta_0 + \\beta_2 + (\\beta_1 + \\beta_3) X_{1i} + e_i.\\] 将其和上面的\\(Y_{i\\_rural}\\)进行对比。虚拟编码起作用了！ 我们如何在R中估计回归系数？假设我们想检验两条线的斜率是否不同。注意，这实际上只是检验原假设\\(\\beta_3 = 0\\)，因为\\(\\beta_3\\)是我们的斜率偏移量。如果这个参数为0，意味着两个组具有相同的斜率(尽管它们可以有不同的截距)。换句话说，这意味着两条斜线是平行的。如果它非0，这意味着两个组具有不同的斜率，也就是说，两条斜线不平行。 样本中的平行线 vs. 总体中的平行线 我刚刚提到，两条不平行的线意味着分类预测变量和连续预测变量之间存在交互作用，而平行的线则意味着不存在交互作用。要明确的是，我所说的平行与否是指在总体(population)中的情况。在样本(sample)中的平行与否不仅取决于它们在总体中的情况，还取决于测量和抽样引入的偏差。总体中是平行的线，在样本中却极有可能出现斜率不同的线，尤其是在样本量较小的情况下。 通常，你会对总体中斜率相同与否感兴趣，而不是样本中的情况。因此，你不能仅仅看样本数据的图表就推断出”线不平行，因此存在交互作用”，或相反”线看起来平行，所以没有交互作用”。你必须进行推论统计检验。 当交互项在某个\\(\\alpha\\)水平(如0.05)上具有统计显著性时，你会拒绝交互系数为0的原假设(如\\(H_0: \\beta_3 = 0\\))，这意味着在总体中这些线不是平行的。 然而，交互项不显著并不一定意味着在总体中这些线是平行的。它们可能是平行的，但也可能不是，而你的研究只是缺乏足够的统计效力(power)来检测出差异。 为了获得关于原假设的证据，最好的方法是进行所谓的等效性检验(equivalence test)。在这个检验中你试图拒绝一个原假设，该原假设是总体效应大于某个你感兴趣的最小效应值；教程详见(Lakens et al., 2018)。 我们已经创建了结合两组模拟数据的数据集all_data。我们使用R公式语法表示模型的方式是Y ~ X1 + X2 + X1:X2，其中X1:X2告诉R创建一个预测变量，该预测变量是X1和X2的乘积。还有一个缩写方式Y ~ X1 * X2，它告诉R计算所有可能的主效应和交互效应。首先，我们将向模型添加一个虚拟预测变量，并将结果存储在all_data2中。 all_data2 &lt;- all_data %&gt;% mutate(grp = if_else(group == &quot;rural&quot;, 1, 0)) sonic_mod &lt;- lm(simple_rt ~ dist_level + grp + dist_level:grp, all_data2) summary(sonic_mod) ## ## Call: ## lm(formula = simple_rt ~ dist_level + grp + dist_level:grp, data = all_data2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -261.130 -50.749 3.617 62.304 191.211 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 460.1098 15.5053 29.674 &lt; 2e-16 *** ## dist_level 1.9123 0.2620 7.299 7.07e-12 *** ## grp 4.8250 21.7184 0.222 0.824 ## dist_level:grp 1.5865 0.3809 4.166 4.65e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 81.14 on 196 degrees of freedom ## Multiple R-squared: 0.5625, Adjusted R-squared: 0.5558 ## F-statistic: 83.99 on 3 and 196 DF, p-value: &lt; 2.2e-16 练习 在下面的空格中输入你的答案，至少保留两位小数。 给定回归模型 \\[Y_{i} = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 X_{1i} X_{2i} + e_{i}\\] (其中\\(X_{1i}\\)是连续预测变量，\\(X_{2i}\\)是分类预测变量)且上方给出了lm()的输出结果，填写以下参数的估计值。 \\(\\hat{\\beta}_0\\): \\(\\hat{\\beta}_1\\): \\(\\hat{\\beta}_2\\): \\(\\hat{\\beta}_3\\): 根据这些参数估计，(基线)城市组的回归线为: \\(Y_i =\\) \\(+\\) \\(X_{1i}\\) 乡村组的： \\(Y_i =\\) \\(+\\) \\(X_{1i}\\) 展开答案 \\(\\beta_0=\\) 460.11 \\(\\beta_1=\\) 1.91 \\(\\beta_2=\\) 4.83 \\(\\beta_3=\\) 1.59 城市组的回归线是： \\(Y_i = \\beta_0 + \\beta_1 X_{1i}\\)即 \\(Y_i =\\) 460.11 \\(+\\) 1.91 \\(X_{1i}\\) 乡村组的是： \\(Y_i = \\beta_0 + \\beta_2 + \\left(\\beta_1 + \\beta_3\\right) X_{1i}\\)即 \\(Y_i=\\) 464.93 \\(+\\) 3.5 \\(X_{1i}\\) 4.2 分类变量-分类变量交互作用 因子设计(factorial design)在心理学中很常见，通常使用基于ANOVA的技术进行分析，这可能掩盖了ANOVA与回归一样也假设了一个潜在的线性模型的事实。 因子是一种所有预测变量(自变量，IVs)都是分类变量的设计：每个都是具有固定数量水平的因子。在一个全因子设计(full-factorial design)中，因子之间完全交叉，以表示每种可能的因子组合。我们称每个唯一的组合为设计的一个单元(cell)。你经常会听到设计被称为“二乘二(two-by-two)设计”(2x2)，这意味着有2个因子，每个因子有2个水平。一个“三乘三设计”(3x3)是其中有2个因子，每个因子有3个水平的设计；一个“二乘二乘二设计”(2x2x2)是其中有3个因子，每个因子有2个水平，以此类推。 通常，因子设计都以表格形式给出，显示所有因子水平的组合。下面是一个2x2设计的表格表示。 \\(B_1\\) \\(B_2\\) \\(A_1\\) \\(AB_{11}\\) \\(AB_{12}\\) \\(A_2\\) \\(AB_{21}\\) \\(AB_{22}\\) 一个3x2设计可能如下所示。 \\(B_1\\) \\(B_2\\) \\(A_1\\) \\(AB_{11}\\) \\(AB_{12}\\) \\(A_2\\) \\(AB_{21}\\) \\(AB_{22}\\) \\(A_3\\) \\(AB_{31}\\) \\(AB_{32}\\) 最后是一个2x2x2设计. \\[C_1\\] \\(B_1\\) \\(B_2\\) \\(A_1\\) \\(ABC_{111}\\) \\(ABC_{121}\\) \\(A_2\\) \\(ABC_{211}\\) \\(ABC_{221}\\) \\[C_2\\] \\(B_1\\) \\(B_2\\) \\(A_1\\) \\(ABC_{112}\\) \\(ABC_{122}\\) \\(A_2\\) \\(ABC_{212}\\) \\(ABC_{222}\\) 参考文献 Lakens, D., Scheel, A. M., &amp; Isager, P. M. (2018). Equivalence testing for psychological research: A tutorial. Advances in Methods and Practices in Psychological Science, 1, 259–269. https://journals.sagepub.com/doi/abs/10.1177/2515245918770963 "],["参考文献.html", "参考文献", " 参考文献 Lakens, D., Scheel, A. M., &amp; Isager, P. M. (2018). Equivalence testing for psychological research: A tutorial. Advances in Methods and Practices in Psychological Science, 1, 259–269. https://journals.sagepub.com/doi/abs/10.1177/2515245918770963 Vanhove, J. (2021). Collinearity isn’t a disease that needs curing. Meta-Psychology, 5. Yarkoni, T. (2019). The generalizability crisis. https://doi.org/10.31234/osf.io/jqw35 "]]
